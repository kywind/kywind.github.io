<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaifeng Zhang</title>

  <meta name="author" content="Kaifeng Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/ucsd-logo.png"> -->
</head>
<script async type="text/javascript" src="static/js/index.js"></script>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:75%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kaifeng Zhang</name>
                  </p>
                  <p>
                    I am a second-year Ph.D. student in computer science at <a href="https://columbia.edu/">Columbia University</a>, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>. 
                    I obtained by Bachelors degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> <a href="https://iiis.tsinghua.edu.cn/en/">(Yao Class)</a>, where I worked with <a href="https://yang-gao.weebly.com/">Prof. Yang Gao</a> 
                    and <a href="https://ericyi.github.io/">Prof. Li Yi</a>, and interned at <a href="https://ucsd.edu/">UC San Diego</a> with <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a>.
                    Prior to joining Columbia, I spent one year at <a href="https://illinois.edu/">UIUC</a>, where I was fortunate to collaborate with <a href="https://kkhauser.web.illinois.edu/">Prof. Kris Hauser</a>.
                  </p>
                  <p>
                    My research interest is at the intersection of robotics, computer vision and machine learning.
                    My research goal is to develop robot systems that perceive and interact with the physical world robustly and generalizably. 
                    Driven by this goal, I focus on topics including 3D reconstruction, physics model learning, and deformable object manipulation.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:kaifeng.z@columbia.edu">Email</a> &nbsp/&nbsp
                    <!--<a href="files/resume.pdf">CV</a> &nbsp/&nbsp-->
                    <a href="https://scholar.google.com/citations?user=jwkE2lgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/kywind/">Github</a> &nbsp/&nbsp
                    <a href="https://twitter.com/kaiwynd">Twitter</a>&nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/kaifeng-zhang-7b971625b/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:0%;width:25%;max-width:25%">
                  <img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/profile2.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                      <li>
                        [2024.9] One paper gets accepted to CoRL 2024.
                      </li>
                      <li>
                        [2024.8] After a wonderful year at UIUC, I will be joining Columbia University to continue my PhD.
                      </li>
                      <li>
                        [2024.5] <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a> is selected as the Best Abstract Award (Top 1) at the 
                        <a href="https://deformable-workshop.github.io/icra2024/">4th RMDO Workshop @ ICRA2024</a>.
                      </li>
                      <li>
                        [2024.5] <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a> is accepted to RSS 2024.
                      </li>
                      <li>
                        [2023.8] Starting my PhD at UIUC, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>.
                      </li>
                      <li>
                        [2023.6] Graduated from Tsinghua University.
                      </li>
                      <li>
                        [2023.1] One paper gets accepted to ICLR 2023.
                      </li>
                      <li>
                        [2022.7] One paper gets accepted to ECCV 2022.
                      </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/adaptigraph.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2407.07889">
                    <papertitle>
                      AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kaifeng Zhang*</strong>,
                  Baoyu Li, 
                  Kris Hauser, 
                  Yunzhu Li
                  <br>
                  Robotics: Science and Systems (RSS), 2024
                  <br>
                  ICRA RMDO Workshop, 2024 <strong><font color='#b70000'>(Best Abstract Award)</strong></font>
                  <br>
                  <a href="https://robopil.github.io/adaptigraph/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2407.07889">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2407.07889.pdf">pdf</a>
                  /
                  <a href="https://github.com/Boey-li/AdaptiGraph">code</a>
                </td>
              </tr>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/self-pose.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2210.07199">
                    <papertitle>
                      Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kaifeng Zhang</strong>,
                  Yang Fu, 
                  Shubhankar Borse, 
                  Hong Cai, 
                  Fatih Porikli,
                  Xiaolong Wang
                  <br>
                  International Conference on Learning Representations (ICLR), 2023
                  <br>
                  <a href="https://kywind.github.io/self-pose">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2210.07199">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2210.07199.pdf">pdf</a>
                  /
                  <a href="https://github.com/kywind/self-corr-pose">code</a>
                  <!-- <p>
                    We propose a self-supervised method for category-level 6D object pose estimation 
                    by learning dense 2D-3D geometric correspondences. Our method can train on in-the-wild RGB/RGBD image collections
                    without any 3D annotations. 
                  </p> -->
                </td>
              </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/sfc-full.png' width="300px">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2207.10456">
                    <papertitle>
                      Semantic-Aware Fine-Grained Correspondence
                    </papertitle>
                  </a>
                  <br>
                  Yingdong Hu, 
                  Renhao Wang, 
                  <strong>Kaifeng Zhang</strong>,
                  Yang Gao
                  <br>
                  European Conference on Computer Vision (ECCV), 2022 <strong><font color='#b70000'>(Oral)</font></strong>
                  <br>
                  <a href="https://arxiv.org/abs/2207.10456">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2207.10456.pdf">pdf</a>
                  /
                  <a href="https://github.com/Yingdong-Hu/SFC">code</a>
                  <!-- <p>
                    We show that fine-grained features learned with low-level contrastive objectives are 
                    complementary to semantic features from image-level SSL methods. Fusing these features can significantly improve the performance for downstream tasks.
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding: 0px">
                  <br>
                  <p style="text-align:right;font-size:16px">
                    Template borrowed from <a href="https://jonbarron.info/" style="font-size:16px">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
</body>

</html>