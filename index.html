<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaifeng Zhang</title>

  <meta name="author" content="Kaifeng Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/ucsd-logo.png"> -->
</head>
<script async type="text/javascript" src="static/js/index.js"></script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kaifeng Zhang</name>
                  </p>
                  <p>
                    I am a Ph.D. student in computer science at Columbia University, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>.
                  </p>
                  
                  <p>
                    My research interest is at the intersection of robotics, machine learning and computer vision. 
                    I am interested in using learning-based methods to make robots perceive and understand the physical world robustly and generalizably.
                  <p> 
                    I am very fortunate to have worked with <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a> from UC San Diego, 
                    and <a href="https://yang-gao.weebly.com/">Prof. Yang Gao</a> from Tsinghua University.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:kaifeng.z@columbia.edu">Email</a> &nbsp/&nbsp
                    <!--<a href="files/resume.pdf">CV</a> &nbsp/&nbsp-->
                    <a href="https://scholar.google.com/citations?user=jwkE2lgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/kywind/">Github</a> &nbsp/&nbsp
                    <a href="https://twitter.com/kaiwynd">Twitter</a><!-- &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/kaifeng-zhang-7b971625b/">Linkedin</a>-->
                  </p>
                </td>
                <td style="padding:0%;width:25%;max-width:25%">
                  <img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/profile2.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Updates</heading>
                  <p>
                      <li>
                        [2024.8] After a wonderful year at UIUC, I will be joining Columbia University to continue persuing my PhD.
                      </li>
                      <li>
                        [2024.5] <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a> is accepted to RSS 2024.
                      </li>
                      <li>
                        [2023.8] Starting my PhD at UIUC, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>.
                      </li>
                      <li>
                        [2023.6] Graduated from Tsinghua University.
                      </li>
                      <li>
                        [2023.1] One paper gets accepted to ICLR 2023.
                      </li>
                      <li>
                        [2022.7] One paper gets accepted to ECCV 2022.
                      </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/adaptigraph-min.jpg' width="200">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2407.07889">
                    <papertitle>
                      AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kaifeng Zhang*</strong>,
                  Baoyu Li, 
                  Kris Hauser, 
                  Yunzhu Li
                  <br>
                  <em>Robotics: Science and Systems (RSS)</em>, 2024;
                  <br>
                  <em>ICRA RMDO Workshop</em>, 2024. <strong><font color='#b70000'>(Best Abstract Award)</strong></font>
                  <br>
                  <a href="https://robopil.github.io/adaptigraph/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2407.07889">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2407.07889.pdf">pdf</a>
                  /
                  <a href="https://github.com/Boey-li/AdaptiGraph">code</a>
                </td>
              </tr>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/self-pose-min.png' width="200">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2210.07199">
                    <papertitle>
                      Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kaifeng Zhang</strong>,
                  Yang Fu, 
                  Shubhankar Borse, 
                  Hong Cai, 
                  Fatih Porikli,
                  Xiaolong Wang
                  <br>
                  <em>International Conference on Learning Representations (ICLR)</em>, 2023.
                  <br>
                  <a href="https://kywind.github.io/self-pose">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2210.07199">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2210.07199.pdf">pdf</a>
                  /
                  <a href="https://github.com/kywind/self-corr-pose">code</a>
                  <!-- <p>
                    We propose a self-supervised method for category-level 6D object pose estimation 
                    by learning dense 2D-3D geometric correspondences. Our method can train on in-the-wild RGB/RGBD image collections
                    without any 3D annotations. 
                  </p> -->
                </td>
              </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/sfc-min.jpg' width="200">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2207.10456">
                    <papertitle>
                      Semantic-Aware Fine-Grained Correspondence
                    </papertitle>
                  </a>
                  <br>
                  Yingdong Hu, 
                  Renhao Wang, 
                  <strong>Kaifeng Zhang</strong>,
                  Yang Gao
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2022. <strong><font color='#b70000'>(Oral)</font></strong>
                  <br>
                  <a href="https://arxiv.org/abs/2207.10456">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2207.10456.pdf">pdf</a>
                  /
                  <a href="https://github.com/Yingdong-Hu/SFC">code</a>
                  <!-- <p>
                    We show that fine-grained features learned with low-level contrastive objectives are 
                    complementary to semantic features from image-level SSL methods. Fusing these features can significantly improve the performance for downstream tasks.
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding: 0px">
                  <br>
                  <p style="text-align:right">
                    Template borrowed from <a href="https://jonbarron.info/">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
</body>

</html>