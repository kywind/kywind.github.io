<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaifeng Zhang</title>

  <meta name="author" content="Kaifeng Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<script async type="text/javascript" src="static/js/index.js"></script>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:75%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kaifeng Zhang</name>
                  </p>
                  <p>
                    I am a Ph.D. student in computer science at <a href="https://columbia.edu/">Columbia University</a>, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>. 
                    Prior to this, I obtained my Bachelors degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> <a href="https://iiis.tsinghua.edu.cn/en/">(Yao Class)</a>. 
                    I am fortunate to receive mentorship from <a href="https://kkhauser.web.illinois.edu/">Prof. Kris Hauser</a> during my Ph.D. study, and <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a>, 
                    <a href="https://yang-gao.weebly.com/">Prof. Yang Gao</a>, <a href="https://ericyi.github.io/">Prof. Li Yi</a> during my undergrad.
                  </p>
                  <p>
                    My research sits at the intersection of robotics, 3D vision, physics simulation, and machine learning. 
                    I am interested in bridging the gap between robotic simulation and the real world for robust and scalable robot manipulation.
                  </p>
                  <p>
                    If you'd like to discuss research opportunity, collaboration, Ph.D. application, or anything related, feel free to reach out via email: 
                    kaifeng dot z at columbia dot edu.
                  </p>
                  <!-- <p> 
                    <b style="color:rgb(183, 0, 0)">Seeking for a research intern position summer 2025. Please let me know if you would like to chat! </b>
                  </p> -->
                  <p style="text-align:center">
                    <a href="https://scholar.google.com/citations?user=jwkE2lgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/kywind/">Github</a> &nbsp/&nbsp
                    <a href="https://twitter.com/kaiwynd">Twitter</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/kaifeng-zhang-7b971625b/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://kywind.github.io/pdf/resume-20251110.pdf">CV</a>
                  </p>
                </td>
                <td style="padding:0%;width:25%;max-width:25%">
                  <img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/profile3.jpg" class="hoverZoomLink"></a>
                  <p style="text-align:center;font-size:14px">
                    Photo credit: <a href="https://bingjietang718.github.io/" style="font-size:14px">Bingjie Tang</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading> &nbsp;&nbsp;
                  (<a href="" id="select0" onclick="showPubs(0); return false;">show selected</a> /
                  <a href="" id="select1" onclick="showPubs(1); return false;">show all</a>)
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_selected" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/real2sim-eval.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://kywind.github.io/">
                    <papertitle>
                      Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a>*</strong>,
                  <a href="https://shuosha.github.io/">Shuo Sha</a>*,
                  <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>,
                  <a href="https://scholar.google.com/citations?user=xokChDMAAAAJ&hl=en">Matthew Loper</a>,
                  <a href="https://sites.google.com/nyu.edu/jay-hyunjong-song">Hyunjong Song</a>,
                  <a href="https://guangyancai.me/">Guangyan Cai</a>,
                  <a href="https://drzhuoxu.github.io/">Zhuo Xu</a>,
                  <a href="https://scholar.google.com/citations?user=-MaXMRAAAAAJ&hl=en">Xiaochen Hu</a>,
                  <a href="https://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>,
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  ArXiv, 2025
                  <br>
                  <a href="https://real2sim-eval.github.io/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2511.04665">arXiv</a>
                  /
                  <a href="https://arxiv.org/abs/2511.04665.pdf">pdf</a>
                  /
                  <a href="https://github.com/kywind/real2sim-eval">code</a>
                  <p>
                    We propose a framework for robot policy evaluation in simulation environments, 
                    using Gaussian Splatting for rendering and soft-body digital twin for dynamics.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_selected" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/phystwin.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://jianghanxiao.github.io/phystwin-web/">
                    <papertitle>
                      PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, 
                  <a href="https://haoyuhsu.github.io/">Hao-Yu Hsu</a>, 
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://www.linkedin.com/in/hnyu/">Hsin-Ni Yu</a>, 
                  <a href="https://shenlong.web.illinois.edu/">Shenlong Wang</a>, 
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  International Conference on Computer Vision (ICCV), 2025
                  <br>
                  <a href="https://jianghanxiao.github.io/phystwin-web/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2503.17973">arXiv</a>
                  /
                  <a href="https://arxiv.org/abs/2503.17973.pdf">pdf</a>
                  /
                  <a href="https://github.com/Jianghanxiao/PhysTwin">code</a>
                  <p>
                    We optimize a spring-mass physics model of deformable objects and 
                    integrate the model with 3D Gaussian Splatting for real-time re-simulation with rendering.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_selected" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/pgnd.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://kywind.github.io/">
                    <papertitle>
                      Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://boey-li.github.io/">Baoyu Li</a>, 
                  <a href="https://kkhauser.web.illinois.edu/">Kris Hauser</a>, 
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  Robotics: Science and Systems (RSS), 2025
                  <br>
                  <a href="https://kywind.github.io/pgnd">website</a>
                  /
                  <a href="https://arxiv.org/abs/2506.15680">arXiv</a>
                  /
                  <a href="https://arxiv.org/abs/2506.15680.pdf">pdf</a>
                  /
                  <a href="https://github.com/kywind/pgnd">code</a>
                  /
                  <a href="https://huggingface.co/spaces/kaifz/pgnd">demo</a>
                  <p>
                    We propose a neural particle-grid model for training dynamics model with real-world sparse-view RGB-D videos, enabling 
                    high-quality future prediction and rendering.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_all" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/gaussian-gbnd.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://gs-dynamics.github.io/">
                    <papertitle>
                      Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://robo-alex.github.io/">Mingtong Zhang</a>*, 
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a>*</strong>,
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  Conference on Robot Learning (CoRL), 2024
                  <br>
                  <a href="https://gs-dynamics.github.io/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2410.18912">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2410.18912.pdf">pdf</a>
                  /
                  <a href="https://github.com/robo-alex/gs-dynamics">code</a>
                  /
                  <a href="https://huggingface.co/spaces/kaifz/gs-dynamics">demo</a>
                  <p>
                  We learn neural dynamics models of objects from real perception data 
                  and combine the learned model with 3D Gaussian Splatting for action-conditioned predictive rendering.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_selected" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/adaptigraph.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://robopil.github.io/adaptigraph/">
                    <papertitle>
                      AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a>*</strong>,
                  <a href="https://boey-li.github.io/">Baoyu Li</a>*, 
                  <a href="https://kkhauser.web.illinois.edu/">Kris Hauser</a>, 
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  Robotics: Science and Systems (RSS), 2024
                  <br>
                  ICRA RMDO Workshop, 2024 <strong><font color='#b70000'>(Best Abstract Award)</strong></font>
                  <br>
                  <a href="https://robopil.github.io/adaptigraph/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2407.07889">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2407.07889.pdf">pdf</a>
                  /
                  <a href="https://github.com/Boey-li/AdaptiGraph">code</a>
                  <p>
                    We learn a material-conditioned neural dynamics model using graph neural network to 
                    enable predictive modeling of diverse real-world objects and achieve efficient manipulation via model-based planning.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_all" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/4drecons.jpg' width="300px">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.10167">
                    <papertitle>
                      4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a single RGB-D Camera with Geometrical and Topological Regularizations
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://xy-cong.github.io/">Xiaoyan Cong</a>,
                  <a href="https://yanghtr.github.io/">Haitao Yang</a>,
                  <a href="https://scholar.google.com/citations?user=ppaEV-8AAAAJ&hl=en">Liyan Chen</a>,
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://ericyi.github.io/">Li Yi</a>,
                  <a href="https://www.cs.utexas.edu/~bajaj/cvc/index.shtml">Chandrajit Bajaj</a>,
                  <a href="https://www.cs.utexas.edu/~huangqx/index.html">Qixing Huang</a>
                  <br>
                  Arxiv, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.10167">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2406.10167.pdf">pdf</a>
                  <p>
                    We achieve 4D neural implicit reconstruction from only a single-view scan 
                    using deformation and topology regularizations.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table class="pub_all" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/self-pose-2.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://kywind.github.io/self-pose">
                    <papertitle>
                      Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://oasisyang.github.io/">Yang Fu</a>, 
                  <a href="https://scholar.google.com/citations?user=ZsgWCyMAAAAJ&hl=en">Shubhankar Borse</a>, 
                  <a href="https://scholar.google.com/citations?user=9y3Kd3cAAAAJ&hl=en">Hong Cai</a>, 
                  <a href="https://www.porikli.com/">Fatih Porikli</a>,
                  <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>
                  <br>
                  International Conference on Learning Representations (ICLR), 2023
                  <br>
                  <a href="https://kywind.github.io/self-pose">website</a>
                  /
                  <a href="https://arxiv.org/abs/2210.07199">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2210.07199.pdf">pdf</a>
                  /
                  <a href="https://github.com/kywind/self-corr-pose">code</a>
                  <p>
                    We propose a fully self-supervised method for category-level 6D object pose estimation 
                    by learning dense 2D-3D geometric correspondences. Our method can train on image collections
                    without any 3D annotations. 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="pub_all" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/sfc-full.png' width="300px">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2207.10456">
                    <papertitle>
                      Semantic-Aware Fine-Grained Correspondence
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://yingdong-hu.github.io/">Yingdong Hu</a>, 
                  <a href="https://renwang435.github.io/">Renhao Wang</a>, 
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                  <br>
                  European Conference on Computer Vision (ECCV), 2022 <strong><font color='#b70000'>(Oral)</font></strong>
                  <br>
                  <a href="https://arxiv.org/abs/2207.10456">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2207.10456.pdf">pdf</a>
                  /
                  <a href="https://github.com/Yingdong-Hu/SFC">code</a>
                  <p>
                    We show that fusing fine-grained features learned with low-level contrastive objectives and semantic features 
                    from image-level objectives can improve SSL pretraining.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Contact</heading>
                  <p>
                    If you are interested in my work and would like to discuss research opportunity, collaboration, Ph.D. application, or anything else, feel free to contact me via email: 
                    kaifeng dot z at columbia dot edu.
                  </p>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding: 0px">
                  <br>
                  <p style="text-align:right;font-size:14px">
                    Template borrowed from <a href="https://jonbarron.info/" style="font-size:14px">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

<script>
function showPubs(mode) {
  const pubs = document.querySelectorAll('.pub_selected, .pub_all');

  pubs.forEach(pub => {
    if (mode === 0) {
      if (pub.classList.contains('pub_selected')) {
        pub.style.display = "";
      } else {
        pub.style.display = "none";
      }
    } else {
      pub.style.display = "";
    }
  });
  document.getElementById("select0").style.fontWeight = mode === 0 ? "bold" : "normal";
  document.getElementById("select1").style.fontWeight = mode === 1 ? "bold" : "normal";
}
</script>
<script>showPubs(0);</script>

</html>