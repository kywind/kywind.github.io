<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaifeng Zhang</title>

  <meta name="author" content="Kaifeng Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/ucsd-logo.png"> -->
</head>
<script async type="text/javascript" src="static/js/index.js"></script>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:75%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kaifeng Zhang</name>
                  </p>
                  <p>
                    I am a second-year Ph.D. student in computer science at <a href="https://columbia.edu/">Columbia University</a>, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>. 
                    I obtained my Bachelors degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> <a href="https://iiis.tsinghua.edu.cn/en/">(Yao Class)</a>, where I worked with <a href="https://yang-gao.weebly.com/">Prof. Yang Gao</a> 
                    and <a href="https://ericyi.github.io/">Prof. Li Yi</a>. I also interned at <a href="https://ucsd.edu/">UC San Diego</a>, collaborating with <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a>.
                    Prior to joining Columbia, I completed a year of graduate study at <a href="https://illinois.edu/">UIUC</a>, where I was fortunate to collaborate with <a href="https://kkhauser.web.illinois.edu/">Prof. Kris Hauser</a>.
                  </p>
                  <p>
                    My research is at the intersection of robotics, computer vision and machine learning. 
                    I am interested in developing robotic systems that perceive and understand the environment in a robust and generalizable manner. 
                    Currently, I explore topics including 3D reconstruction and tracking, neural dynamics model learning, and their applications to object manipulation.
                  </p>
                  <p style="text-align:center">
                    <a href="https://scholar.google.com/citations?user=jwkE2lgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/kywind/">Github</a> &nbsp/&nbsp
                    <a href="https://twitter.com/kaiwynd">Twitter</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/kaifeng-zhang-7b971625b/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:0%;width:25%;max-width:25%">
                  <img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/profile2.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                      <li>
                        [2024.9] One paper gets accepted to CoRL 2024.
                      </li>
                      <li>
                        [2024.8] After a wonderful year at UIUC, I will be joining Columbia University to continue my PhD.
                      </li>
                      <li>
                        [2024.5] <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a> is selected as the Best Abstract Award at the 
                        <a href="https://deformable-workshop.github.io/icra2024/">4th RMDO Workshop @ ICRA2024</a>.
                      </li>
                      <li>
                        [2024.5] <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a> is accepted to RSS 2024.
                      </li>
                      <li>
                        [2023.8] Starting my PhD at UIUC, advised by <a href="https://yunzhuli.github.io/">Prof. Yunzhu Li</a>.
                      </li>
                      <li>
                        [2023.6] Graduated from Tsinghua University.
                      </li>
                      <li>
                        [2023.1] One paper gets accepted to ICLR 2023.
                      </li>
                      <li>
                        [2022.7] One paper gets accepted to ECCV 2022.
                      </li>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/gaussian-gbnd.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://gs-dynamics.github.io/">
                    <papertitle>
                      Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://robo-alex.github.io/">Mingtong Zhang</a>*, 
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a>*</strong>,
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  Conference on Robot Learning (CoRL), 2024
                  <br>
                  <a href="https://gs-dynamics.github.io/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2410.18912">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2410.18912.pdf">pdf</a>
                  /
                  <a href="https://github.com/robo-alex/gs-dynamics">code</a>
                  /
                  <a href="https://huggingface.co/spaces/kaifz/gs-dynamics">demo</a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/adaptigraph.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://robopil.github.io/adaptigraph/">
                    <papertitle>
                      AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a>*</strong>,
                  <a href="https://scholar.google.com/citations?user=pFeywbYAAAAJ&hl=en">Baoyu Li</a>*, 
                  <a href="https://kkhauser.web.illinois.edu/">Kris Hauser</a>, 
                  <a href="https://yunzhuli.github.io/">Yunzhu Li</a>
                  <br>
                  Robotics: Science and Systems (RSS), 2024
                  <br>
                  ICRA RMDO Workshop, 2024 <strong><font color='#b70000'>(Best Abstract Award, Top 1)</strong></font>
                  <br>
                  <a href="https://robopil.github.io/adaptigraph/">website</a>
                  /
                  <a href="https://arxiv.org/abs/2407.07889">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2407.07889.pdf">pdf</a>
                  /
                  <a href="https://github.com/Boey-li/AdaptiGraph">code</a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/4drecons.jpg' width="300px">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.10167">
                    <papertitle>
                      4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a single RGB-D Camera with Geometrical and Topological Regularizations
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://xy-cong.github.io/">Xiaoyan Cong</a>,
                  <a href="https://yanghtr.github.io/">Haitao Yang</a>,
                  <a href="https://scholar.google.com/citations?user=ppaEV-8AAAAJ&hl=en">Liyan Chen</a>,
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://ericyi.github.io/">Li Yi</a>,
                  <a href="https://www.cs.utexas.edu/~bajaj/cvc/index.shtml">Chandrajit Bajaj</a>,
                  <a href="https://www.cs.utexas.edu/~huangqx/index.html">Qixing Huang</a>
                  <br>
                  Arxiv, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.10167">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2406.10167.pdf">pdf</a>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline width="300px">
                    <source src="vids/main/self-pose-2.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://kywind.github.io/self-pose">
                    <papertitle>
                      Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild
                    </papertitle>
                  </a>
                  <br>
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://oasisyang.github.io/">Yang Fu</a>, 
                  <a href="https://scholar.google.com/citations?user=ZsgWCyMAAAAJ&hl=en">Shubhankar Borse</a>, 
                  <a href="https://scholar.google.com/citations?user=9y3Kd3cAAAAJ&hl=en">Hong Cai</a>, 
                  <a href="https://www.porikli.com/">Fatih Porikli</a>,
                  <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>
                  <br>
                  International Conference on Learning Representations (ICLR), 2023
                  <br>
                  <a href="https://kywind.github.io/self-pose">website</a>
                  /
                  <a href="https://arxiv.org/abs/2210.07199">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2210.07199.pdf">pdf</a>
                  /
                  <a href="https://github.com/kywind/self-corr-pose">code</a>
                  <!-- <p>
                    We propose a self-supervised method for category-level 6D object pose estimation 
                    by learning dense 2D-3D geometric correspondences. Our method can train on in-the-wild RGB/RGBD image collections
                    without any 3D annotations. 
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <img src='images/sfc-full.png' width="300px">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2207.10456">
                    <papertitle>
                      Semantic-Aware Fine-Grained Correspondence
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://yingdong-hu.github.io/">Yingdong Hu</a>, 
                  <a href="https://renwang435.github.io/">Renhao Wang</a>, 
                  <strong><a href="https://kywind.github.io/">Kaifeng Zhang</a></strong>,
                  <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                  <br>
                  European Conference on Computer Vision (ECCV), 2022 <strong><font color='#b70000'>(Oral)</font></strong>
                  <br>
                  <a href="https://arxiv.org/abs/2207.10456">arXiv</a>
                  /
                  <a href="https://arxiv.org/pdf/2207.10456.pdf">pdf</a>
                  /
                  <a href="https://github.com/Yingdong-Hu/SFC">code</a>
                  <!-- <p>
                    We show that fine-grained features learned with low-level contrastive objectives are 
                    complementary to semantic features from image-level SSL methods. Fusing these features can significantly improve the performance for downstream tasks.
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Contact</heading>
                  <p>
                    If you are interested in my work and would like to discuss anything, feel free to email me at: kaifeng [dot] z [at] columbia [dot] edu
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding: 0px">
                  <br>
                  <p style="text-align:right;font-size:14px">
                    Template borrowed from <a href="https://jonbarron.info/" style="font-size:14px">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>