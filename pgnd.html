<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos.">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://kywind.github.io">Kaifeng Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://boey-li.github.io/">Baoyu Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://kkhauser.web.illinois.edu/">Kris Hauser</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://yunzhuli.github.io/">Yunzhu Li</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Columbia University</span>&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>University of Illinois Urbana-Champaign</span>
          </div>
          
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b>Robotics: Science and Systems (RSS), 2025</b></span>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.roboticsproceedings.org/rss21/p036.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.15680"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/kywind/pgnd"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://kywind.github.io/pgnd"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>X/Twitter</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Particle-Grid Neural Dynamics is a learning-based digital twin framework for deformable objects, trained on real RGB-D videos and operable directly on raw observations.
      </h2>
      <video autoplay muted loop playsinline height="100%">
        <source src="vids/pgnd/teaser.mp4" type="video/mp4">
      </video>
    </div>
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Try our Huggingface interactive demo of PGND simulating 3D Gaussians:
        <br>
        <span class="link-block">
          <a href="https://huggingface.co/spaces/kaifz/pgnd"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">ðŸ¤—</span>
            <span>Interactive Demo</span>
          </a>
        </span>
      </h2>
      <div class="columns is-centered has-text-centered">
      </div>
      <iframe
          src="https://kaifz-pgnd.hf.space"
          frameborder="0"
          width="850"
          height="450"
      ></iframe>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
          <p>
            Modeling the dynamics of deformable objects is challenging due to their diverse physical properties 
            and the difficulty of estimating states from limited visual information. We address these challenges 
            with a neural dynamics framework that combines object particles and spatial grids in a hybrid 
            representation. Our particle-grid model captures global shape and motion information while predicting 
            dense particle movements, enabling the modeling of objects with varied shapes and materials. 
            Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial 
            continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, 
            our framework achieves a fully learning-based digital twin of deformable objects and generates 
            3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics 
            of diverse objectsâ€”such as ropes, cloths, stuffed animals, and paper bagsâ€”from sparse-view RGB-D 
            recordings of robot-object interactions, while also generalizing at the category level to unseen instances. 
            Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly 
            in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in 
            model-based planning, enabling goal-conditioned object manipulation across a range of tasks.
           </p>
          </div>
        </div>
      </div>
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/SZThFL6gfXg"
              frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    
    <h2 class="title is-3">Motivation</h3>
      <p> 
        Simulating deformable objects like cloths and ropes is hard because of their 
        complex physics and partial observability. In this work, 
        we overcome these challenges by learning a neural model for object dynamics 
        directly from real-world videos.
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <video autoplay muted loop playsinline>
            <source src="vids/pgnd/motivation.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    <h2 class="title is-3">Method</h3>
    <div class="content has-text-justified">
      <p> 
        The particle-based neural dynamics model represents objects as dense 3D particles 
        and predicts their next-step velocities to simulate the object dynamics.
        It consists of three stages: particle encoding, grid velocity editing, and grid-to-particle velocity transfer.
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/pgnd/method.mp4" type="video/mp4">
        </video> 
      </div>
    </div>


  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Results</h2>
    <div class="columns">
      <div class="column">
        <p>
          We evaluate our method on 6 diverse deformable object categories, including ropes, cloths, stuffed animals, and paper bags.
          Our model is trained separately on each category using less than 20 minutes of RGB-D videos of robot-object interactions.
        </p>
      </div>
    </div>

    <h3 class="title is-4">Future Prediction</h3>
    <div class="columns">
      <div class="column">
        <p>
          Given initial states and actions, we show the prediction results of the GBND
          baseline compared to our particle-grid neural dynamics model. We
          overlay the predictions with ground truth final state images to highlight the prediction errors. PGND's predictions are more aligned with
          the ground truth, offering higher-density particle predictions and fewer artifacts compared to the baseline.
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/pgnd/particle.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <h3 class="title is-4">3D Action-Conditioned Video Prediction</h3>
    <div class="columns">
      <div class="column">
        <p>
          When plugged into a Gaussian Splatting renderer, PGND can generate high-quality 3D action-conditioned videos.
          PGND's results aligns better with
          the ground truth while the SOTA baseline method predicts visually nonrealistic deformations.
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/pgnd/gs.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    
    <h3 class="title is-4">Simulation with 3D Gaussians</h3>
    <div class="columns">
      <div class="column">
        <p>
          PGND can serve as a deformable object simulator given Gaussian Splatting reconstructions of the scene. 
          Given only the initial static reconstruction, we apply PGND to simulate the segmented object given a sequence of actions (red arrows).
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/pgnd/hq.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <h3 class="title is-4">Model-Based Planning</h3>
    <div class="columns">
      <div class="column">
        <p>
          PGND can be integrated with MPC to generate actions for manipulating objects.
          We test on 4 tasks with distinct object types: cloth lifting, box
          closing, rope manipulation, and plush toy relocating. In all tasks, our
          method produces results that are closer to the target.
        </p>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/pgnd/planning.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{zhang2024particle,
  title={Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos},
  author={Zhang, Kaifeng and Li, Baoyu and Hauser, Kris and Li, Yunzhu},
  booktitle={Proceedings of Robotics: Science and Systems (RSS)},
  year={2025}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/kywind" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-centered">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>