<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Self-supervised Geometric Correspondence for Category-level 6D Object Pose Estimation in the Wild.">
  <meta name="keywords" content="Vision Transformer, Text Supervision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Self-supervised Geometric Correspondence for Category-level 6D Object Pose Estimation in the Wild</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Self-supervised Geometric Correspondence for Category-level 6D Object Pose Estimation in the Wild</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kywind.github.io">Kaifeng Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://oasisyang.github.io">Yang Fu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://kywind.github.io/self-pose">Shubhankar Borse</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://kywind.github.io/self-pose">Hong Cai</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://kywind.github.io/self-pose">Fatih Porikli</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiaolonw.github.io/">Xiaolong Wang</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>1</sup>UC San Diego,</span>
            <span class="author-block"><sup>2</sup>Qualcomm AI Research</span>
          </div>
          <!--<div class="is-size-6 publication-authors">-->
            <!-- <span class="author-block">(* Jiarui Xu was an intern at NVIDIA during the project)</span> -->
          <!--  <span class="author-block">(* the work was done at an internship at NVIDIA)</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2202.11094.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2202.11094"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://youtu.be/DtJsWIUTW-Y"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="groupvit_slide.key"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-slideshare"></i>
                  </span>
                  <span>Slide</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NVlabs/GroupViT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!--
              <span class="link-block">
                <a href="https://github.com/xvjiarui/GroupViT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Personal Code with Models</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/spaces/xvjiarui/GroupViT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-face-smiling-hands"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>-->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video autoplay muted loop playsinline height="100%">
        <source src="vids/self-pose/teaser.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Without using any 3D annotations, our model estimates the dense 2D-3D geometric correspondence and the 6D pose of objects.
      </h2>
      <video autoplay muted loop playsinline height="100%">
        <source src="vids/self-pose/wild6d-vid.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The video estimation results are generated per-frame.
      </h2>
      </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
          <p>
            While 6D object pose estimation has wide applications across computer vision and robotics, 
            it remains far from being solved due to the lack of annotations. 
            The problem becomes even more challenging when moving to category-level 6D pose, 
            which requires generalization to unseen instances. 
            Current approaches are restricted by leveraging annotations from simulation or collected from humans. 
            In this paper, we overcome this barrier by introducing a self-supervised learning approach 
            trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. 
            Our framework reconstructs the canonical 3D shape of an object category 
            and learns dense correspondences between input images and the canonical shape via surface embedding. 
            For training, we propose novel geometrical cycle-consistency losses which construct 
            cycles across 2D-3D spaces, across different instances and different time steps. 
            The learned correspondence can be applied for 6D pose estimation and other downstream 
            tasks such as keypoint transfer. Surprisingly, our method, without any human annotations or simulators, 
            can achieve on-par or even better performance than previous supervised or semi-supervised methods 
            on in-the-wild images. 
           </p>
          </div>
        </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/IeteYvPkpFk"ywind
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <!--
    <h2 class="title is-3">Problem Overview</h2>
    <div class="columns">
      <div class="column">
        <p>We train a category-level 6D object pose estimation model using in-the-wild video dataset in a self-supervised manner.
          
          With GroupViT, meaningful semantic grouping automatically emerges without any mask annotations. 
          Then, we transfer the trained GroupViT model to the task of zero-shot semantic segmentation.</p>
      </div>
    </div>
    </div>-->
    
    <h2 class="title is-3">Method Overview</h3>
    <div class="content has-text-justified">
      <p> Given the input image and a categorical canonical mesh learned by our model, we extract image per-pixel and mesh per-vertex embeddings, and establish dense geometric correspondences via feature similarity in the embedding space. 
      </p>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/self-pose/method.mp4" type="video/mp4">
        </video> 
      </div>
    </div>

    <h2 class="title is-3">Cycle Consistency</h3>
      <p> 
        We propose novel losses that establish cycle consistency across 2D and 3D space. 
        The instance cycle consistency loss encourages the consistency between correspondence and an estimated camera projection.
        The cross-instance and cross-time cycle consistency loss goes beyond a single image-mesh pair to cross-instance and cross-time 
        images, encouraging category-level semantic consistency.
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <video autoplay muted loop playsinline>
            <source src="vids/self-pose/cc.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <!--
      <p> 
        The cross-instance and cross-time cycle consistency
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <video autoplay muted loop playsinline>
            <source src="vids/self-pose/ccc.mp4" type="video/mp4">
          </video>
        </div>
      </div>-->

  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Results</h2>
    <div class="columns">
      <div class="column">
        <p>
          We evaluate our method on the Wild6D, NOCS-REAL275 and CUB-200-2011. On Wild6D and REAL275, we visualize the correspondence and 
          pose estimation results. The model is trained on Wild6D training set which contains no 3D annotations, 
          and tested on Wild6D test set and REAL275 test set by zero-shot transfer.
          On CUB-200-2011, we visualize the correspondence and keypoint transfer results.
        </p>
      </div>
    </div>

    <h3 class="title is-4">Wild6D</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/self-pose/wild6d.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <h3 class="title is-4">REAL275</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <video autoplay muted loop playsinline>
          <source src="vids/self-pose/nocs.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <h3 class="title is-4">CUB</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <img src="images/cubmatch.png" alt="">
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <img src="images/kp.png" alt="">
      </div>
    </div>

      <!--<video autoplay muted loop playsinline width="100%">
        <source src="figs/website_voc_results.m4v" type="video/mp4">
      </video>  
    <details>
      <summary class="title is-6 button">Click here for more results on Pascal VOC</summary>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_voc_results_more.m4v" type="video/mp4">
      </video>
    </details>

    <h3 class="title is-4">Pascal Context (zero-shot transfer without fine-tuning) </h3>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_ctx_results.m4v" type="video/mp4">
      </video>
    <details>
      <summary class="title is-6 button">Click here for more results on Pascal Context</summary>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_ctx_results_more.m4v" type="video/mp4">
      </video>
    </details>

    <h3 class="title is-4">COCO (zero-shot transfer without fine-tuning)</h3>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_coco_results.m4v" type="video/mp4">
      </video>
    <details>
      <summary class="title is-6 button">Click here for more results on COCO</summary>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_coco_results_more.m4v" type="video/mp4">
      </video>
    </details>-->
  </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xu2022groupvit,
  author    = {Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
  title     = {GroupViT: Semantic Segmentation Emerges from Text Supervision},
  journal   = {arXiv preprint arXiv:2202.11094},
  year      = {2022},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/xvjiarui" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>