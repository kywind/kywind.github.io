<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Self-supervised Geometric Correspondence for Category-level 6D Object Pose Estimation in the Wild.">
  <meta name="keywords" content="Vision Transformer, Text Supervision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Self-supervised Geometric Correspondence for Category-level 6D Object Pose Estimation in the Wild</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Self-supervised Geometric Correspondence for Category-level 6D Object Pose Estimation in the Wild</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kywind.github.io">Kaifeng Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://oasisyang.github.io">Yang Fu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://kywind.github.io/self-pose">Shubhankar Borse</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://kywind.github.io/self-pose">Hong Cai</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://kywind.github.io/self-pose">Fatih Porikli</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiaolonw.github.io/">Xiaolong Wang</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup>Tsinghua University,</span>
            <span class="author-block"><sup>1</sup>University of California San Diego,</span>
            <span class="author-block"><sup>2</sup>Qualcomm</span>
          </div>
          <!--<div class="is-size-6 publication-authors">-->
            <!-- <span class="author-block">(* Jiarui Xu was an intern at NVIDIA during the project)</span> -->
          <!--  <span class="author-block">(* the work was done at an internship at NVIDIA)</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2202.11094.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2202.11094"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://youtu.be/DtJsWIUTW-Y"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="groupvit_slide.key"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-slideshare"></i>
                  </span>
                  <span>Slide</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NVlabs/GroupViT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!--
              <span class="link-block">
                <a href="https://github.com/xvjiarui/GroupViT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Personal Code with Models</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/spaces/xvjiarui/GroupViT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solid fa-face-smiling-hands"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>-->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
<!--       <video autoplay muted loop playsinline height="100%">
        <source src="figs/website_teaser.m4v" type="video/mp4">
        <source src="images/dmcontrol-gb.png" type="image/png">
      </video>  -->
      <img src="images/sample.jpeg" alt="">
      <h2 class="subtitle has-text-centered">
        Without using any 3D annotations like object models or pose, our model estimates the dense geometric correspondence between image and a canonical mesh, and 6D pose of objects.
        <!--<span style="color: orange; font-weight:bold">Without using any mask annotations</span>, GroupVit groups the image into segments and outputs a semantic segmentation map.-->
      </h2>
      </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
          <p>
            While 6D object pose estimation has wide applications across computer vision and robotics, 
            it remains far from being solved due to the lack of annotations. 
            The problem becomes even more challenging when moving to category-level 6D pose, 
            which requires generalization to unseen instances. 
            Current approaches are restricted by leveraging annotations from simulation or collected from humans. 
            In this paper, we overcome this barrier by introducing a self-supervised learning approach 
            trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. 
            Our framework reconstructs the canonical 3D shape of an object category 
            and learns dense correspondences between input images and the canonical shape via surface embedding. 
            For training, we propose novel geometrical cycle-consistency losses which construct 
            cycles across 2D-3D spaces, across different instances and different time steps. 
            The learned correspondence can be applied for 6D pose estimation and other downstream 
            tasks such as keypoint transfer. Surprisingly, our method, without any human annotations or simulators, 
            can achieve on-par or even better performance than previous supervised or semi-supervised methods 
            on in-the-wild images. 
           d</p>
          </div>
        </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/DtJsWIUTW-Y"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Problem Overview</h2>
    <div class="columns">
      <div class="column">
        <p>We train a category-level 6D object pose estimation model using in-the-wild video dataset in a self-supervised manner.
          
          With GroupViT, meaningful semantic grouping automatically emerges without any mask annotations. 
          Then, we transfer the trained GroupViT model to the task of zero-shot semantic segmentation.</p>
      </div>
    </div>
    </div>-->
    
    <h2 class="title is-3">Model Architecture</h3>
    <p> Given the input image, the image encoder and feature extractor predict per-pixel features, and establish dense geometric correspondence with mesh per-vertex features. 2D reconstruction loss and cycle consistency loss provide supervision for the model.
     </p>
    <div class="columns is-centered has-text-centered">
      <div class="column is-fullwidth">
        <img src="images/sample.jpeg" alt="">
        <!-- 
        <video autoplay muted loop playsinline controls>
          <source src="images/dmcontrol-gb.png" type="image/png">
        </video> -->
      </div>
    </div>

    <h2 class="title is-3">Cycle Consistency</h3>
      <p> 
      </p>
      <div class="columns is-centered has-text-centered">
        <div class="column is-fullwidth">
          <img src="images/sample.jpeg" alt="">
          <!-- 
          <video autoplay muted loop playsinline controls>
            <source src="images/dmcontrol-gb.png" type="image/png">
          </video> -->
        </div>
      </div>

  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-widescreen">
    <h2 class="title is-3">Results</h2>
    <div class="columns">
      <div class="column">
        <p></p>
      </div>
    </div>

    <h3 class="title is-4">Wild6D</h3>
    <img src="images/sample.jpeg" alt="">
    <img src="images/sample.jpeg" alt="">

    <h3 class="title is-4">REAL275 (transfer without fine-tuning)</h3>
    <img src="images/sample.jpeg" alt="">

    <h3 class="title is-4">CUB-200-2011</h3>
    <img src="images/sample.jpeg" alt="">

<!--      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_voc_results.m4v" type="video/mp4">
      </video>  
    <details>
      <summary class="title is-6 button">Click here for more results on Pascal VOC</summary>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_voc_results_more.m4v" type="video/mp4">
      </video>
    </details>

    <h3 class="title is-4">Pascal Context (zero-shot transfer without fine-tuning) </h3>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_ctx_results.m4v" type="video/mp4">
      </video>
    <details>
      <summary class="title is-6 button">Click here for more results on Pascal Context</summary>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_ctx_results_more.m4v" type="video/mp4">
      </video>
    </details>

    <h3 class="title is-4">COCO (zero-shot transfer without fine-tuning)</h3>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_coco_results.m4v" type="video/mp4">
      </video>
    <details>
      <summary class="title is-6 button">Click here for more results on COCO</summary>
      <video autoplay muted loop playsinline width="100%">
        <source src="figs/website_coco_results_more.m4v" type="video/mp4">
      </video>
    </details>-->
  </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xu2022groupvit,
  author    = {Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
  title     = {GroupViT: Semantic Segmentation Emerges from Text Supervision},
  journal   = {arXiv preprint arXiv:2202.11094},
  year      = {2022},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/xvjiarui" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>